{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIZy9uSNF0PF",
        "outputId": "3af14a34-752a-49cb-ba01-451fbfa94cbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZTUc41dDb_k"
      },
      "source": [
        "--\n",
        "\n",
        "Environment Description\n",
        "\n",
        "CartPole-v1 : https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
        "\n",
        "Acrobot-v1 : https://gymnasium.farama.org/environments/classic_control/acrobot/\n",
        "\n",
        "MountainCar-v0 : https://gymnasium.farama.org/environments/classic_control/mountain_car/\n",
        "\n",
        "--"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9igj6TrV25J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcmZHZ5oazus"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRwRMsrdZ7xW"
      },
      "source": [
        "## 1. DQN Implemetation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFqviUpRWEUx"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # Define a single hidden layer network\n",
        "        self.fc1 = nn.Linear(state_dim, 128)  # First fully connected layer\n",
        "        self.fc2 = nn.Linear(128, action_dim)  # Output layer\n",
        "\n",
        "    def forward(self, s):\n",
        "        # Pass the input through the layers with ReLU activation\n",
        "        x = torch.relu(self.fc1(s))\n",
        "        q = self.fc2(x)  # Output layer without activation\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZiCFDBwWKmR"
      },
      "outputs": [],
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self, state_size, action_size, device):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.device = device\n",
        "\n",
        "        # Do not modify these hyper-parameters\n",
        "        self.Epochs = 1000\n",
        "        self.discount_factor = 0.98\n",
        "        self.learning_rate = 0.001\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.001\n",
        "        self.batch_size = 256\n",
        "        self.train_start = self.batch_size * 5\n",
        "        self.memory = deque(maxlen=100000)\n",
        "\n",
        "        # You can modify this depending on environments.\n",
        "        self.epsilon_decay_rate = 0.995\n",
        "\n",
        "        # Define and initialize your networks and optimizer\n",
        "        self.q_network = QNetwork(state_size, action_size).to(device)\n",
        "        self.target_network = QNetwork(state_size, action_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=self.learning_rate)\n",
        "\n",
        "        self.update_target_network()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "    def get_action(self, state, use_epsilon_greedy=True):\n",
        "        if use_epsilon_greedy and np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(self.action_size)\n",
        "        else:\n",
        "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            q_values = self.q_network(state)\n",
        "            return torch.argmax(q_values).item()\n",
        "\n",
        "    def append_sample(self, state, action, reward, next_state, done):\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def get_samples(self, n):\n",
        "        samples = random.sample(self.memory, n)\n",
        "        states, actions, rewards, next_states, dones = zip(*samples)\n",
        "\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.LongTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n",
        "\n",
        "    def epsilon_decay(self):\n",
        "        if self.epsilon > self.epsilon_min:\n",
        "            self.epsilon *= self.epsilon_decay_rate\n",
        "\n",
        "    def train(self):\n",
        "        if len(self.memory) < self.train_start:\n",
        "            return None\n",
        "\n",
        "        s_batch, a_batch, r_batch, s_next_batch, done_batch = self.get_samples(self.batch_size)\n",
        "\n",
        "        q_values = self.q_network(s_batch).gather(1, a_batch.unsqueeze(1)).squeeze(1)\n",
        "        next_q_values = self.target_network(s_next_batch).max(1)[0]\n",
        "        target_q_values = r_batch + (self.discount_factor * next_q_values * (1 - done_batch))\n",
        "\n",
        "        loss = nn.MSELoss()(q_values, target_q_values.detach())\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        self.epsilon_decay()\n",
        "\n",
        "        return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRSX1bPeILPu"
      },
      "outputs": [],
      "source": [
        "# Do not modify this cell\n",
        "def evaluation(env, agent):\n",
        "    done, episode_score, episode_steps = False, 0.0, 0\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    for t in range(env._max_episode_steps):\n",
        "        action = agent.get_action(state, use_epsilon_greedy=False)\n",
        "        state_next, reward, done, time_truncation, _ = env.step(action)\n",
        "\n",
        "        episode_score += reward\n",
        "        episode_steps += 1\n",
        "\n",
        "        state = state_next\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return episode_score, episode_steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGps4pJ-R1Vi"
      },
      "outputs": [],
      "source": [
        "# experiments envs : \"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\"\n",
        "\n",
        "env_name          = \"CartPole-v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PCkudfR1XPDL"
      },
      "outputs": [],
      "source": [
        "# Do not modify this cell\n",
        "\n",
        "env               = gym.make(env_name)\n",
        "state_size        = env.observation_space.shape[0]\n",
        "action_size       = env.action_space.n\n",
        "max_episode_steps = env._max_episode_steps\n",
        "device            = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"state_size:{}, action_size:{}, max_episode_steps:{}, device:{} \\n\".format(state_size, action_size, max_episode_steps, device))\n",
        "\n",
        "agent = DQNAgent(state_size, action_size, device)\n",
        "\n",
        "\n",
        "\n",
        "episode_scores_list, episode_steps_list = [], []\n",
        "for epoch in range(agent.Epochs):  # agent.Epochs = 1000\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    for t in range(max_episode_steps):\n",
        "        action = agent.get_action(state)\n",
        "        state_next, reward, done, time_truncation, _ = env.step(action)\n",
        "\n",
        "        agent.append_sample(state, action, reward, state_next, done)\n",
        "\n",
        "        if len(agent.memory) >= agent.train_start:\n",
        "            agent.train()\n",
        "\n",
        "        state = state_next\n",
        "        if done:\n",
        "            state, _ = env.reset()\n",
        "\n",
        "    agent.update_target_network()\n",
        "    agent.epsilon_decay()\n",
        "\n",
        "    episode_score, episode_steps = evaluation(env, agent)\n",
        "    episode_scores_list.append(episode_score)\n",
        "    episode_steps_list.append(episode_steps)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch:{}, Episode_score:{}, Episode_steps:{}, epsilon:{}\".format(epoch, episode_score, episode_steps, agent.epsilon))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLmlHNfySvlC"
      },
      "outputs": [],
      "source": [
        "# This cell provides basic evaluation score graph. You can add more plots.\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(episode_scores_list)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Episode Scores\")\n",
        "plt.title(\"Episode Scores of {}\".format(env_name))\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(episode_steps_list)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Episode Steps\")\n",
        "plt.title(\"Episode Steps of {}\".format(env_name))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxfGlM7WaxSt"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSbN7TChaDwF"
      },
      "source": [
        "## 2. Double DQN implemantation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOSfIAoMa7Y0"
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super(QNetwork, self).__init__()\n",
        "        # input  : (b_size, state_dim)\n",
        "        # output : (b_size, action_dim)\n",
        "\n",
        "    def forward(self, s):\n",
        "        #\n",
        "\n",
        "        return q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHP4E72fCmN9"
      },
      "outputs": [],
      "source": [
        "class DoubleDQNAgent:\n",
        "    def __init__(self, state_size, action_size, device):\n",
        "        self.state_size         = state_size\n",
        "        self.action_size        = action_size\n",
        "        self.device             = device\n",
        "\n",
        "        # Do not modify these hyper-parameters\n",
        "        self.Epochs             = 1000\n",
        "        self.discount_factor    = 0.98\n",
        "        self.learning_rate      = 0.001  # learning rate for q function\n",
        "        self.epsilon            = 1.0    # initial epsilon value\n",
        "        self.epsilon_min        = 0.001  # minimum epsilon value\n",
        "        self.batch_size         = 256\n",
        "        self.train_start        = self.batch_size * 5\n",
        "        self.memory             = deque(maxlen=100000)  # replay memory\n",
        "\n",
        "        # You can modify this depending on environments.\n",
        "        self.epsilon_decay_rate =    # decay rate\n",
        "\n",
        "        # Define and initialize your networks and optimizer\n",
        "\n",
        "\n",
        "\n",
        "    def update_target_network(self):\n",
        "        # implement target Q network update function\n",
        "\n",
        "\n",
        "    def get_action(self, state, use_epsilon_greedy=True):\n",
        "        if use_epsilon_greedy:\n",
        "            # implement epsilon greedy policy given state\n",
        "        else:\n",
        "            # implement greedy policy given state\n",
        "            # this greedy policy is used for evaluation\n",
        "\n",
        "        return action\n",
        "\n",
        "    def append_sample(self, state, action, reward, next_state, done):\n",
        "        # implement storing function given (s,a,r,s',done) into the replay memory.\n",
        "\n",
        "    def get_samples(self, n):\n",
        "        # implement transition random sampling function from the replay memory,\n",
        "        # and make the transiton to batch.\n",
        "\n",
        "        # i.e.) s_batch : (batch_size, state_dim)\n",
        "        return s_batch, a_batch, r_batch, s_next_batch, done_batch\n",
        "\n",
        "    def epsilon_decay(self):\n",
        "        # implement epsilon decaying function that\n",
        "\n",
        "    def train(self):\n",
        "        s_batch, a_batch, r_batch, s_next_batch, done_batch = self.get_samples(self.batch_size)\n",
        "\n",
        "        # implement Double DQN training function.\n",
        "        # You can return any statistics you want to check and analize the training. (i.e. loss, q values, target q values, ... etc)\n",
        "        return None\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gAgh33TCDA2h"
      },
      "outputs": [],
      "source": [
        "# Do not modify this cell\n",
        "def evaluation(env, agent):\n",
        "    done, episode_score, episode_steps = False, 0.0, 0\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    for t in range(env._max_episode_steps):\n",
        "        action = agent.get_action(state, use_epsilon_greedy=False)\n",
        "        state_next, reward, done, time_truncation, _ = env.step(action)\n",
        "\n",
        "        episode_score += reward\n",
        "        episode_steps += 1\n",
        "\n",
        "        state = state_next\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return episode_score, episode_steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sb-x88JXDFOj"
      },
      "outputs": [],
      "source": [
        "# experiments envs : \"CartPole-v1\", \"Acrobot-v1\", \"MountainCar-v0\"\n",
        "\n",
        "env_name          = \"CartPole-v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaoxqTozDHzQ"
      },
      "outputs": [],
      "source": [
        "# Do not modify this cell\n",
        "\n",
        "env               = gym.make(env_name)\n",
        "state_size        = env.observation_space.shape[0]\n",
        "action_size       = env.action_space.n\n",
        "max_episode_steps = env._max_episode_steps\n",
        "device            = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "print(\"state_size:{}, action_size:{}, max_episode_steps:{}, device:{} \\n\".format(state_size, action_size, max_episode_steps, device))\n",
        "\n",
        "agent = DoubleDQNAgent(state_size, action_size, device)\n",
        "\n",
        "\n",
        "\n",
        "episode_scores_list, episode_steps_list = [], []\n",
        "for epoch in range(agent.Epochs):  # agent.Epochs = 1000\n",
        "\n",
        "    state, _ = env.reset()\n",
        "    for t in range(max_episode_steps):\n",
        "        action = agent.get_action(state)\n",
        "        state_next, reward, done, time_truncation, _ = env.step(action)\n",
        "\n",
        "        agent.append_sample(state, action, reward, state_next, done)\n",
        "\n",
        "        if len(agent.memory) >= agent.train_start:\n",
        "            agent.train()\n",
        "\n",
        "        state = state_next\n",
        "        if done:\n",
        "            state, _ = env.reset()\n",
        "\n",
        "    agent.update_target_network()\n",
        "    agent.epsilon_decay()\n",
        "\n",
        "    episode_score, episode_steps = evaluation(env, agent)\n",
        "    episode_scores_list.append(episode_score)\n",
        "    episode_steps_list.append(episode_steps)\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch:{}, Episode_score:{}, Episode_steps:{}, epsilon:{}\".format(epoch, episode_score, episode_steps, agent.epsilon))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6tUwNL_DN-P"
      },
      "outputs": [],
      "source": [
        "# This cell provides basic evaluation score graph. You can add more plots.\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(episode_scores_list)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Episode Scores\")\n",
        "plt.title(\"Episode Scores of {}\".format(env_name))\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(episode_steps_list)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Episode Steps\")\n",
        "plt.title(\"Episode Steps of {}\".format(env_name))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4NDsu7qa-oV"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOCjHuyybArE"
      },
      "source": [
        "## 3. Additional Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAoU9KWqbE4m"
      },
      "outputs": [],
      "source": [
        "# You can implement any RL algorithm that could increase performance, but experiment on the three environments."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
